<!DOCTYPE html>
    <title>Lifelong Robotics</title>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-0evHe/X+R7YkIZDRvuzKMRqM+OrBnVFBL6DOitfPri4tjfHxaWutUpFmBp4vmVor" crossorigin="anonymous">
    <link rel="stylesheet" href="styles.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LRLBHJD1CS"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-LRLBHJD1CS');
    </script>
</head>
<body>
    <div class="container" style="max-width: 720px">
        <header class="row mt-1 mb-1 border border-secondary text-center bg-info rounded">
            <h1>Lifelong Learning of High-level Cognitive and Reasoning Skills</h1>
            <span>A Full Day Hybrid Workshop at <a href="https://iros2022.org">IROS 2022</a><br><a href="https://calendar.google.com/event?action=TEMPLATE&tmeid=MnY2NHFjcGloaWRpZHQ0YzhxZjIwdjJsMnEgYWhtZXRvZ2x1LmFscGVyQG0&tmsrc=ahmetoglu.alper%40gmail.com"><time datetime="2022-10-23 09:00+09:00">9:00AM - 5:00 PM (UTC+9), October 23, 2022</time></a></span>
        </header>
        <div class="row bg-light rounded border border-secondary">
            <nav class="navbar-nav">
                <ul class="nav align-self-center">
                    <li class="nav-link"><a class="btn btn-outline-secondary" href="#about">About</a></li>
                    <li class="nav-link"><a class="btn btn-outline-secondary" href="#invited">Invited Speakers</a></li>
                    <li class="nav-link"><a class="btn btn-outline-secondary" href="#schedule">Schedule</a></li>
                    <li class="nav-link"><a class="btn btn-outline-secondary" href="#cfp">Call for Papers</a></li>
                    <li class="nav-link"><a class="btn btn-outline-secondary" href="#organizing">Organizing</a></li>
                </ul>
            </nav>
        </div>
        <div class="row" id="news"><h2>News</h2></div>
        <div class="row">
            <div class="col">
            <ul class="">
                <li>Join our <a href="https://join.slack.com/t/llr2022/shared_invite/zt-1f9qfyzck-YDkXU55gEQDi0yLnv8zY2g">Slack channel</a> for more information.</li>
            </ul>
            </div>
        </div>
        <div class="row" id="about"><h2>About</h2></div>
        <div class="row">
            <div class="col">
            <p>There has been a tremendous advancement in machine learning methods over the past ten years. These advancements are reflected in robotics research primarily as ad-hoc solutions to different problems, such as learning to grasp simple objects <a href="#r1">[1]</a>, <a href="#r2">[2]</a>, learning movement primitives from demonstrations <a href="#r3">[3]</a>, <a href="#r4">[4]</a> and so on. Although such solutions push the frontiers of robotics research, there is still a large gap between current methods and generally capable agents that learn skills and representations of progressive complexity to solve increasingly complex problems in an environment without forgetting previously learned skills, which is the ultimate aim of artificial intelligence research. Several studies <a href="#r5">[5]</a>, <a href="#r6">[6]</a>, <a href="#r7">[7]</a>, <a href="#r8">[8]</a> focus on learning representations in an open-ended, continual manner in order to develop general skills without manually designing reward functions or curricula. The main advantage of open-ended (or lifelong, or continual) learning is that the learning loop never stops by design; the agent autonomously generates new challenges, tasks, environments, and goals to explore and learn new skills. This opens new possibilities for learning emergent skills not foreseen in hindsight. However, these methods do not precisely fit robot learning as we cannot arbitrarily evolve the embodiment of the agent and the real world.</p>
            <p>This workshop will focus on how to create open-ended or lifelong learning systems <a href="#r9">[9]</a>, <a href="#r10">[10]</a> that will allow for a robot to autonomously explore its environment and learn ever-growing representations for perception and actuation. This is an underexplored area in robotics and artificial intelligence research that can contribute to the development of generally capable agents. We will discuss the necessary elements (e.g., methods, environments, datasets, embodiments) for a lifelong learning setting. We will encourage participants to take part in discussions with the following points:
            <ul>
                <li>How to adopt the current open-ended methods for lifelong robot learning</li>
                <li>Are the current machine learning methods sufficient for lifelong learning?</li>
                <li>How to combine the current toolset of ML for lifelong learning</li>
                <li>How to design end-to-end systems for lifelong learning</li>
                <li>What are the necessary components for a lifelong learning system? Which parts should we take for granted?</li>
                <li>How to design appropriate environments both in simulation and real-world that support lifelong learning</li>
            </ul></p>
            <p>We believe that approaching robot learning from a lifelong learning perspective might be a feasible approach for truly intelligent robots. Stimulating discussions with the robotics community will be very valuable to understand if such an approach is indeed worth exploring or not.</p>
            </div>
        </div>
        <div class="row" id="invited"><h2>Invited Speakers</h2></div>
        <div class="row my-1">
            <div class="col-3"><img src="images/george.jpeg" class="img-fluid rounded"><a href="https://cs.brown.edu/people/gdk/">George Konidaris</a>, Brown University</div>
            <div class="col-3"><img src="images/jeff.jpeg" class="img-fluid rounded"><a href="http://jeffclune.com">Jeff Clune</a>, OpenAI, University of British Columbia</div>
            <div class="col-3"><img src="images/jorge.jpeg" class="img-fluid rounded"><a href="https://jorge-a-mendez.github.io">Jorge Mendez</a>, MIT CSAIL</div>
            <div class="col-3"><img src="images/jun.jpeg" class="img-fluid rounded"><a href="https://groups.oist.jp/cnru/jun-tani">Jun Tani</a>, Okinawa Institute of Science and Technology</div>
        </div>
        <div class="row my-1">
            <div class="col-3"><img src="images/minoru.jpeg" class="img-fluid rounded"><a href="http://www.er.ams.eng.osaka-u.ac.jp/asadalab/?page_id=330&lang=en">Minoru Asada</a>, Osaka University</div>
            <div class="col-3"><img src="images/stefanie.jpeg" class="img-fluid rounded"><a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a>, Brown University</div>
            <div class="col-3"><img src="images/tamim.jpeg" class="img-fluid rounded"><a href="https://scholar.google.com/citations?user=65bIT4oAAAAJ&hl=en">Tamim Asfour</a>, Karlsruhe Institute of Technology</div>
            <div class="col-3"><img src="images/yukie.jpeg" class="img-fluid rounded"><a href="https://developmental-robotics.jp/en/members/yukie_nagai/">Yukie Nagai</a>, University of Tokyo</div>
        </div>
        <div class="row" id="schedule"><h2>Schedule</h2></div>
        <div class="row bg-light">
            <div class="col">
                <table class="table">
                    <tbody>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">8:50-9:00 (JST, Oct 23)</span> &mdash; <span style="color: green;">1:50-2:00 (CEST, Oct 23)</span> &mdash; <span style="color: blue">19:50-20:00 (EDT, Oct 22)</span></b></p>
                                Opening remarks
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">9:00-9:45 (JST, Oct 23)</span> &mdash; <span style="color: green;">2:00-2:45 (CEST, Oct 23)</span> &mdash; <span style="color: blue">20:00-20:45 (EDT, Oct 22)</span></b></p>
                                <span><b>Speaker:</b> Jeff Clune</span><br>
                                <p><b>Title:</b> Improving Robot and Deep Reinforcement Learning via Quality Diversity, Open-Ended, and AI-Generating Algorithms</p>
                                <p><b>Abstract:</b> Quality Diversity (QD) algorithms are those that seek to produce a diverse set of high-performing solutions to problems. I will describe them and a number of their positive attributes. I will summarize how they enable robots, after being damaged, to adapt in 1-2 minutes in order to continue performing their mission. I will next describe our QD-based Go-Explore algorithm, which dramatically improves the ability of deep reinforcement learning algorithms to solve previously unsolvable problems wherein reward signals are sparse, meaning that intelligent exploration is required. Go-Explore solved all unsolved Atari games, including Montezuma's Revenge and Pitfall, considered by many to be a grand challenges of AI research. I will next motivate research into open-ended algorithms, which seek to innovate endlessly, and introduce our POET algorithm, which generates its own training challenges while learning to solve them, automatically creating a curricula for robots to learn an expanding set of diverse skills. Finally, I'll argue that an alternate paradigm &mdash;AI-generating algorithms (AI-GAs)&mdash; may be the fastest path to accomplishing our field's grandest ambition of creating general AI, and describe how QD and Open-Ended algorithms will be essential ingredients of AI-GAs.</p>
                                <p><b>Bio:</b> Jeff Clune is an Associate Professor of computer science at the University of British Columbia and a faculty member at the Vector Institute. Jeff focuses on deep learning, including deep reinforcement learning. Previously he was a research manager at OpenAI, a Senior Research Manager and founding member of Uber AI Labs (formed after Uber acquired a startup he helped lead), the Harris Associate Professor in Computer Science at the University of Wyoming, and a Research Scientist at Cornell University. He received degrees from Michigan State University (PhD, master's) and the University of Michigan (bachelor's). More on Jeff's research can be found at <a href="http://jeffclune.com">JeffClune.com</a> or on Twitter (<a href="https://twitter.com/jeffclune">@jeffclune</a>).<br>
                                Since 2015, he won the Presidential Early Career Award for Scientists and Engineers from the White House, had two papers in Nature and one in PNAS, won an NSF CAREER award, received Outstanding Paper of the Decade and Distinguished Young Investigator awards, and had best paper awards, oral presentations, and invited talks at the top machine learning conferences (NeurIPS, CVPR, ICLR, and ICML). His research is regularly covered in the press, including the New York Times, NPR, NBC, Wired, the BBC, the Economist, Science, Nature, National Geographic, the Atlantic, and the New Scientist.</p>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">9:45-10:30 (JST, Oct 23)</span> &mdash; <span style="color: green;">2:45-3:30 (CEST, Oct 23)</span> &mdash; <span style="color: blue">20:45-21:30 (EDT, Oct 22)</span></b></p>
                                <span><b>Speaker:</b> Stefanie Tellex</span>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">10:30-11:15 (JST, Oct 23)</span> &mdash; <span style="color: green;">3:30-4:15 (CEST, Oct 23)</span> &mdash; <span style="color: blue">21:30-22:15 (EDT, Oct 22)</span></b></p>
                                <span><b>Speaker:</b> Jorge Mendez</span><br>
                                <p><b>Title:</b> Lifelong Robot Learning via Functional Compositionality</p>
                                <p><b>Abstract:</b> In order to be deployed long-term in the real world, machine learning systems must be able to handle the one thing that is constant: change. Traditional machine learning focuses on stationary tasks, and is therefore incapable of adapting to change. Instead, we need lifelong learners that can accumulate knowledge that enables them to rapidly adjust to new tasks. Ideally, pieces of this accumulated knowledge could be composed in different ways to adapt to the shifting environment. This capability would dramatically improve the performance of machine learning systems in dynamic environments: hate-speech detection models could adapt to social media trends, student feedback software could adjust to new cohorts, search-and-rescue robots could handle novel disasters.<br>
                                Our research develops algorithms for lifelong or continual learning that leverage the intuition that accumulated knowledge should be compositional. In this talk, I will walk through some of the algorithms that we have developed for lifelong supervised and reinforcement learning, and I will show that these compositional methods enable far improved lifelong learning in settings where tasks are highly diverse. I will dive into the important distinction between temporal and functional composition, and describe its relevance towards lifelong robotics. I will finally introduce CompoSuite, a newly released benchmark for lifelong and multitask robot learning that studies the notion of functionally compositional reinforcement learning, and discuss some open questions that can be studied with CompoSuite.</p>
                                <p><b>Bio:</b> Jorge Mendez is a Postdoctoral Fellow at the Massachusetts Institute of Technology, in the Computer Science and Artificial Intelligence Laboratory (CSAIL). He obtained his Ph.D. from the University of Pennsylvania (UPenn), in the Department of Computer and Information Science and the General Robotics, Automation, Sensing, and Perception (GRASP) Lab. He received a Master's degree in Robotics from UPenn (2018) and a Bachelor's degree in Electronics Engineering from Universidad Simon Bolivar in Venezuela (2016). His primary interests lie in the creation of versatile artificially intelligent systems that learn to accumulate knowledge over their lifetimes. His research leverages a breadth of techniques from transfer learning, multitask learning, compositional learning, representation learning, and reinforcement learning to develop algorithms with applications in computer vision, robotics, and natural language. His work has been recognized with the Best Paper Award in the Lifelong Machine Learning Workshop (ICML, 2020), the third place prize of the Two Sigma Ph.D. Diversity Fellowship (2021), and the School of Engineering Postdoctoral Fellowship for Engineering Excellence from MIT (2022).</p>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">11:15-12:00 (JST, Oct 23)</span> &mdash; <span style="color: green;">4:15-5:00 (CEST, Oct 23)</span> &mdash; <span style="color: blue">22:15-23:00 (EDT, Oct 22)</span></b></p>
                                <span><b>Speaker:</b> George Konidaris</span><br>
                                <p><b>Title:</b> Reintegrating AI: Skills, Symbols, and the Sensorimotor Dilemma</p>
                                <p><b>Abstract:</b> I will address the question of how a robot should learn an abstract, task-specific representation of an environment. I will present a constructivist approach, where the computation the representation is required to support - here, planning using a given set of motor skills - is precisely defined, and then its properties are used to build the representation so that it is capable of doing so by construction. The result is a formal link between the skills available to a robot and the symbols it should use to plan with them. I will present an example of a robot autonomously learning a (sound and complete) abstract representation directly from sensorimotor data, and then using it to plan. I will also discuss ongoing work on making the resulting abstractions portable across tasks.</p>
                                <p><b>Bio:</b> George Konidaris is an Associate Professor of Computer Science at Brown and the Chief Roboticist of Realtime Robotics, a startup commercializing his work on hardware-accelerated motion planning. He holds a BScHons from the University of the Witwatersrand, an MSc from the University of Edinburgh, and a PhD from the University of Massachusetts Amherst. Prior to joining Brown, he held a faculty position at Duke and was a postdoctoral researcher at MIT. George is the recent recipient of an NSF CAREER award, young faculty awards from DARPA and the AFOSR, and the IJCAI-JAIR Best Paper Prize.</p>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">12:00-13:00 (JST, Oct 23)</span> &mdash; <span style="color: green;">5:00-6:00 (CEST, Oct 23)</span> &mdash; <span style="color: blue">23:00-00:00 (EDT, Oct 22)</span></b></p>
                                Lunch Break
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">13:00-14:00 (JST, Oct 23)</span> &mdash; <span style="color: green;">6:00-7:00 (CEST, Oct 23)</span> &mdash; <span style="color: blue">00:00-01:00 (EDT, Oct 23)</span></b></p>
                                Contributed talks
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">14:00-14:45 (JST, Oct 23)</span> &mdash; <span style="color: green;">7:00-7:45 (CEST, Oct 23)</span> &mdash; <span style="color: blue">01:00-01:45 (EDT, Oct 23)</span></b></p>
                                <span><b>Speaker:</b> Tamim Asfour</span><br>
                                <p><b>Title:</b> Continual Learning for Personal Humanoid Robotics</p>
                                <p><b>Abstract:</b> Personal humanoid robots that should coexist with humans to assist and collaborate with them should be able to continually learn from interaction with the world and their users to extend the representations needed to reason about the world. The talk will present progress towards 24/7 humanoid robots &mdash; as embodied AI systems &mdash; that learn from experience and verbalize such experience, learn object relations and acquire task representations for manipulation skills from small number of human demonstration videos. The talk will conclude with a discussion of current research directions. </p>
                                <p><b>Bio:</b> Tamim Asfour is full Professor of Humanoid Robotics at the Institute for Anthropomatics and Robotics at the Karlsruhe Institute of Technology (KIT), Germany. His research focuses on the engineering of high performance 24/7 humanoid robotics. In particular, he studies the mechano-informatics of humanoids as the synergetic integration of informatics, artificial intelligence, and mechatronics into complete humanoid robot systems, which are able perform versatile tasks real world. Tamim is the developer of the ARMAR humanoid robot family. He has been a visiting professor at Georgia Tech, at the Tokyo University of Agriculture and Technology, and at the National University of Singapore. He is Editor-in-Chief and Editor of the Robotics and Automation Letters (RA-L), the Founding Editor-in-Chief of the IEEE-RAS Humanoids Conference Editorial Board, the president of the Executive Board of the German Robotics Society (DGR), and the scientific spokesperson of the KIT Center "Information &middot; Systems &middot; Technologies" (KCIST), <a href="http://www.humanoids.kit.edu">www.humanoids.kit.edu</a></p>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">14:45-15:30 (JST, Oct 23)</span> &mdash; <span style="color: green;">7:45-8:30 (CEST, Oct 23)</span> &mdash; <span style="color: blue">01:45-02:30 (EDT, Oct 23)</span></b></p>
                                <span><b>Speaker:</b> Jun Tani</span><br>
                                <p><b>Title:</b> Dynamic Goal-directed Planning and Goal Understanding of Robots by Extended Active Inference</p>
                                <p><b>Abstract:</b> This study shows that goal-directed action planning and generation in a teleological framework can be formulated by extending the active inference framework. The proposed model, which is built on a variational recurrent neural network model, is characterized by three essential features. These are that (1) goals can be specified for both static sensory states, e.g., for goal images to be reached and dynamic processes, e.g., for moving around an object, (2) the model cannot only generate goal-directed action plans, but can also understand goals through sensory observation, and (3) the model generates future action plans for given goals based on the best estimate of the current state, inferred from past sensory observations. The proposed model is evaluated by conducting experiments on a real humanoid robot performing object manipulation. I'll also briefly describe the current on-going study on dynamic replanning against interferences and incremental tutoring for recovery.</p>
                                <p><b>Bio:</b> Jun Tani received the D.Eng. degree from Sophia University, Tokyo in 1995. He started his research career with Sony Computer Science Lab. in 1993. He became a Team Leader of the Laboratory for Behavior and Dynamic Cognition, RIKEN Brain Science Institute, Saitama, Japan in 2001. He became a Full Professor with the Electrical Engineering Department, Korea Advanced Institute of Science and Technology, Daejeon, South Korea in 2012. He is currently a Full Professor with the Okinawa Institute of Science and Technology, Okinawa, Japan. He is also a visiting professor of The Technical University of Munich. His current research interests include cognitive neuroscience, developmental psychology, phenomenology, complex adaptive systems, and robotics. He is an author of "Exploring Robotic Minds: Actions, Symbols, and Consciousness as Self-Organizing Dynamic Phenomena." published from Oxford Univ. Press in 2016.</p>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">15:30-16:15 (JST, Oct 23)</span> &mdash; <span style="color: green;">8:30-9:15 (CEST, Oct 23)</span> &mdash; <span style="color: blue">02:30-03:15 (EDT, Oct 23)</span></b></p>
                                <span><b>Speaker:</b> Minoru Asada</span><br>
                                <p><b>Title:</b> How Human Brain Develops in the Process of Self-face Recognition, Bodily Awareness and Self-awareness</p>
                                <p><b>Abstract:</b> In this talk, we will explain how human brain develops in the process of self-face recognition, bodily awareness and self-awareness based on fMRI studies. First, we have shown that similar to slow maturation of neural processing for face recognition, neural substrates of self-face recognition slowly developed in typically developing humans. Next, the developmental process of bodily awareness showed U-shape change from children to adults, more correctly, suppression of the left-sided IPL activity is found in adolescents. Finally, we examined the development and aging of brain deactivation using a unimanual motor task, and found that brain inhibition develops and declines through aging. Based on these studies, we discuss how to design life-long learning robots.</p>
                                <p><b>Bio:</b> Minoru Asada received his B.E. (1977), M.E. (1979), and Ph.D. (1982) in Control Engineering from Osaka University, Osaka, Japan. He became a full Professor of Mechanical Engineering for Computer-Controlled Machinery at Osaka University in 1995. Since April 1997, he has been a Professor at the Department of Adaptive Machine Systems at Osaka University (Suita, Japan). Since 2013, he has been the director of the division of cognitive neuroscience robotics, the Institute for Academic Initiatives (IAI), Osaka University. He was the Research Director of the JST (Japan Science and Technology Agency) ERATO (Exploratory Research for Advanced Technology) ASADA Synergistic Intelligence Project during 2005 and 2012. In 2012, the Japan Society for Promotion of Science (JSPS) named him to serve as the Research Leader for the Specially Promoted Research Project (Tokusui) on Constructive Developmental Science Based on Understanding the Process From Neuro-Dynamics to Social Interaction.</p>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <p><b><span style="color: red;">16:15-17:00 (JST, Oct 23)</span> &mdash; <span style="color: green;">9:15-10:00 (CEST, Oct 23)</span> &mdash; <span style="color: blue">03:15-04:00 (EDT, Oct 23)</span></b></p>
                                <span><b>Speaker:</b> Yukie Nagai</span><br>
                                <p><b>Title:</b> The Emergence of Social Cognition through Open-Ended Predictive Learning</p>
                                <p><b>Abstract:</b> Human children acquire social cognitive abilities through open-ended learning. They start reading the internal state of others (e.g., intention and emotion) and cooperating with others in the first few years of life despite no explicit learning of social skills. An open question is where their social abilities and social motivation come from.<br>
                                My talk will present our computational approach based on the neuroscience theory of predictive coding. Our key idea is that the internal model acquired through sensorimotor experiences allows a robot to infer the internal state of others as if their own internal state and that a discrepancy between the predicted sensory state and the actual state produced by others triggers the robot to execute an action to minimize the prediction error. I will show our robot experiments that demonstrate how social cognition emerges through open-ended predictive learning.</p>
                                <p><b>Bio:</b> Yukie Nagai is a Project Professor at the International Research Center for Neurointelligence, the University of Tokyo. She received her Ph.D. in Engineering from Osaka University in 2004 and then worked at the National Institute of Information and Communications Technology, Bielefeld University, and Osaka University. Since 2019, she leads Cognitive Developmental Robotics Lab at the University of Tokyo. Her research interests include cognitive developmental robotics, computational neuroscience, and assistive technologies for developmental disorders. She has been investigating underlying neural mechanisms for social cognitive development by means of computational approaches. She was elected to "30 women in robotics you need to know about" in 2019 and "World's 50 Most Renowned Women in Robotics" in 2020.</p>
                            </td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
        <div class="row" id="cfp"><h2>Call for Papers</h2></div>
        <div class="row">
            <div class="col">
                <p>We accept <b>regular papers</b> (up to 8 pages of unpublished work) and <b>extended abstracts</b> (up to 4 pages of novel work or from a recently published paper) in the standard IEEE conference format, excluding unlimited pages for references. The review process will be single-blind. 10 minutes will be allocated for each regular paper, and 2 minutes for each extended abstract. <b>Papers will be submitted through CMT</b> (<a href="https://cmt3.research.microsoft.com/LLR2022">https://cmt3.research.microsoft.com/LLR2022</a>).</p>
                <p>The topics that are indicative but by no means exhaustive are as follows:</p>
                <ul>
                    <li>Open-ended learning, lifelong learning</li>
                    <li>Continual learning, transfer learning, catastrophic forgetting</li>
                    <li>Multi-task learning, meta-learning</li>
                    <li>Modularity</li>
                    <li>Computational approaches to the study of development and learning</li>
                    <li>Machine learning techniques for robot learning and development</li>
                    <li>Cognitive and perceptual development</li>
                    <li>Concept formation and symbol grounding</li>
                    <li>Imitation learning</li>
                    <li>Embodied cognition</li>
                    <li>Affordance learning</li>
                    <li>Interactive learning</li>
                    <li>Exploration and learning in animals and robots</li>
                    <li>Curiosity and intrinsic motivation</li>
                    <li>Skill and language acquisition</li>
                    <li>Human assisted learning and scaffolding</li>
                    <li>Human-robot interaction</li>
                </ul>
                <h5>Important dates</h5>
                <ul>
                    <s><li>Paper Submission Deadline: <time datetime="2022-08-29">Aug 29, 2022</time></li></s>
                    <s><li>Acceptance Notification: <time datetime="2022-09-02">Sep 2, 2022</time></li></s>
                    <s><li>Paper Submission Deadline: <time datetime="2022-09-30">Sep 30, 2022</time></li></s>
                    <s><li>Acceptance Notification: <time datetime="2022-10-07">Oct 7, 2022</time></li></s>
                    <li>Workshop date: <time datetime="2022-10-23">Oct 23, 2022</time></li>
                </ul>
            </div>
        </div>

        <div class="row" id="organizing"><h2>Organizing Team</h2></div>
        <div class="row">
            <div class="row my-1"> 
                <div class="col-2"><img src="images/alper.jpeg" class="img-fluid rounded"></div>
                <div class="col-4"><a href="https://alpera.xyz">Alper Ahmetoglu</a> (main contact)<br>Bogazici University, Turkey</div>
                <div class="col-2"><img src="images/tuluhan.jpeg" class="img-fluid rounded"></div>
                <div class="col-4"><a href="https://scholar.google.com/citations?user=Zddf-sEAAAAJ&hl=en">Mete Tuluhan Akbulut</a><br> Brown University, Rhodes Island (US)</div>
            </div>
            <div class="row my-1"> 
                <div class="col-2"><img src="images/erhan.jpeg" class="img-fluid rounded"></div>
                <div class="col-4"><a href="https://robotics.ozyegin.edu.tr/members/erhan-oztop/">Erhan Oztop</a><br>Ozyegin University, Turkey<br>Osaka University, Japan</div>
                <div class="col-2"><img src="images/justus.jpeg" class="img-fluid rounded"></div>
                <div class="col-4"><a href="https://iis.uibk.ac.at/people/justus">Justus Piater</a><br>University of Innsbruck, Austria</div>
            </div>
            <div class="row my-1"> 
                <div class="col-2"><img src="images/tadahiro.jpeg" class="img-fluid rounded"></div>
                <div class="col-4"><a href="https://www.tanichu.com">Tadahiro Taniguchi</a><br>Ritsumeikan University, Japan</div>
                <div class="col-2"><img src="images/emre.jpeg" class="img-fluid rounded"></div>
                <div class="col-4"><a href="https://www.cmpe.boun.edu.tr/~emre/">Emre Ugur</a><br>Bogazici University, Turkey</div>
            </div>
        </div>
        <div class="row"><h3>References</h3></div>
        <div class="row">
            <div class="col">
                <ol>
                    <li><a id="r1"></a>Pinto, Lerrel, and Abhinav Gupta. "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours." 2016 IEEE international conference on robotics and automation (ICRA). IEEE, 2016.</li>
                    <li><a id="r2"></a>Murali, Adithyavairavan, et al. "Learning to grasp without seeing." International Symposium on Experimental Robotics. Springer, Cham, 2018.</li>
                    <li><a id="r3"></a>Paraschos, Alexandros, et al. "Probabilistic movement primitives." Advances in neural information processing systems 26 (2013).</li>
                    <li><a id="r4"></a>Seker, Muhammet Yunus, et al. "Conditional Neural Movement Primitives." Robotics: Science and Systems. Vol. 10. 2019.</li>
                    <li><a id="r5"></a>Wang, Rui, et al. "Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions." arXiv preprint arXiv:1901.01753 (2019).</li>
                    <li><a id="r6"></a>Wang, Rui, et al. "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions." International Conference on Machine Learning. PMLR, 2020.</li>  
                    <li><a id="r7"></a>Team, Open Ended Learning, et al. "Open-ended learning leads to generally capable agents." arXiv preprint arXiv:2107.12808 (2021).</li>
                    <li><a id="r8"></a>Parker-Holder, Jack, et al. "Evolving Curricula with Regret-Based Environment Design." arXiv preprint arXiv:2203.01302(2022).</li>
                    <li><a id="r9"></a>Thrun, Sebastian, and Tom M. Mitchell. "Lifelong robot learning." Robotics and autonomous systems 15.1-2 (1995): 25-46.</li>
                    <li><a id="r10"></a>Oztop, Erhan, and Emre Ugur. (2021) Lifelong Robot Learning. In: Ang M.H., Khatib O., Siciliano B. (eds) Encyclopedia of Robotics. Springer, Berlin, Heidelberg, 2021.</li>
                </ol>
            </div>
        </div>
    </div>
</body>
